{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import copy\n",
    "\n",
    "# set the font size\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "# set Helvetica globally\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "# silence future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "plot_folder = '../plots/1_process_count_data/'\n",
    "data_output_folder = '../measured_data/2_normalized_log10/'\n",
    "\n",
    "# create the plot folder if it doesn't exist\n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder)\n",
    "    \n",
    "# create the data output folder if it doesn't exist\n",
    "if not os.path.exists(data_output_folder):\n",
    "    os.makedirs(data_output_folder)\n",
    "\n",
    "cell_lines_subset = [\"HEK293T\", \"HeLa\", \"SKNSH\", \"MCF7\", \"HUH7\", \"A549\"]\n",
    "cell_lines_rest = [\"HaCaT\", \"JEG3\", \"Tera1\", \"PC3\"]\n",
    "cell_lines = cell_lines_subset + cell_lines_rest\n",
    "\n",
    "cell_line_labels = [f\"{cell_line}_3UTR\" for cell_line in cell_lines]\n",
    "\n",
    "label_rename = {\n",
    "    \"HUH-7\": \"HUH7\",\n",
    "    \"JEG-3\": \"JEG3\",\n",
    "    \"Tera-1\": \"Tera1\",\n",
    "    \"SK-N-SH\": \"SKNSH\",\n",
    "    \"PC-3\": \"PC3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebooks processes raw stability data into normalized stability data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Interreplicate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.read_csv('../measured_data/1_count_data/library2_count_and_log2fc_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all columns that contain \"count\"\n",
    "count_cols = [col for col in count_df.columns if 'count' in col]\n",
    "\n",
    "# of these, get the ones that contain \"r1\"\n",
    "r1_count_cols = [col for col in count_cols if 'r1' in col]\n",
    "# of these, get the ones that contain \"r2\"\n",
    "r2_count_cols = [col for col in count_cols if 'r2' in col]\n",
    "\n",
    "# get all indices that contain \"miRNA\"\n",
    "mirna_idx = [idx for idx in count_df.index if 'miRNA' in idx or 'mirna' in idx]\n",
    "\n",
    "# get controls\n",
    "control_idx = [idx for idx in count_df.index if '0_lib2_control' in idx]\n",
    "\n",
    "# drop all data that is not a control or miRNA design\n",
    "count_df = count_df.loc[control_idx + mirna_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only columns that contain \"count\"\n",
    "count_df_filter = count_df[count_cols]\n",
    "\n",
    "# drop the count_\n",
    "count_df_filter.columns = [col.replace('count_', '') for col in count_df_filter.columns]\n",
    "\n",
    "# drop the 3UTR_ from the column names\n",
    "count_df_filter.columns = [col.replace('3UTR_', '') for col in count_df_filter.columns]\n",
    "\n",
    "# make it log10\n",
    "count_df_filter = np.log10(count_df_filter)\n",
    "\n",
    "# calculate the correlation between all columns\n",
    "corr_df = count_df_filter.corr()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot folder if it doesn't exist\n",
    "if not os.path.exists(os.path.join(plot_folder, '1.1_interreplicate_correlation')):\n",
    "    os.makedirs(os.path.join(plot_folder, '1.1_interreplicate_correlation'))\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.set(font_scale=0.7)\n",
    "ax = sns.heatmap(corr_df, annot=False, fmt=\".2f\", cmap='viridis', square=True, cbar_kws={'label': r'R$^2$', 'shrink': 0.8})\n",
    "\n",
    "# Set the cell line labels, positioning them in the middle of their replicates\n",
    "ax.set_xticks([2*i+1 for i in range(len(cell_lines)+1)])\n",
    "ax.set_xticklabels([\"DNA\"] + cell_lines, rotation=90)\n",
    "ax.set_yticks([2*i+1 for i in range(len(cell_lines)+1)])\n",
    "ax.set_yticklabels([\"DNA\"] + cell_lines, rotation=0)\n",
    "\n",
    "plt.title('Interreplicate correlation within library 2', fontsize=8)\n",
    "\n",
    "for format in ['svg', 'png']:\n",
    "    plt.savefig(os.path.join(plot_folder, '1.1_interreplicate_correlation/library2_3UTR_correlation.' + format), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete NaNs in the count df\n",
    "count_df = count_df.dropna()\n",
    "\n",
    "# reset matplotlib after using seaborn\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "# set the font size\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "# set Helvetica globally\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "n = len(r2_count_cols)\n",
    "rows = int(np.ceil(np.sqrt(n)))\n",
    "cols = int(np.ceil(n / rows))\n",
    "\n",
    "# Create a subplot grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(1.4*cols, 1.2*rows))\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "if rows > 1 and cols > 1:\n",
    "    axs = axs.ravel()\n",
    "\n",
    "for i in range(n):\n",
    "    r1_col = r1_count_cols[i]\n",
    "    r2_col = r2_count_cols[i]\n",
    "    r1_vals = np.log10(count_df[r1_col] + 1)\n",
    "    r2_vals = np.log10(count_df[r2_col] + 1)\n",
    "    \n",
    "    ax = axs[i]\n",
    "    ax.scatter(r1_vals, r2_vals, s=10, alpha=0.5, linewidths=0, color='grey', rasterized=True)\n",
    "    \n",
    "    # calculate the r2 value\n",
    "    r2 = stats.pearsonr(r1_vals, r2_vals)[0]**2\n",
    "    \n",
    "    # Set x and y axis labels only for specific positions\n",
    "    if i >= (rows - 1) * cols:\n",
    "        ax.set_xlabel(r\"log$_{10}$(counts rep1)\", fontsize=7)\n",
    "    if i % cols == 0:\n",
    "        ax.set_ylabel(\"log$_{10}$(counts rep2)\", fontsize=7)\n",
    "\n",
    "    ax.set_xlim(0.5, 4.5)\n",
    "    ax.set_ylim(0.5, 4.5)\n",
    "    ax.set_xticks([1,2,3,4])\n",
    "    ax.set_yticks([1,2,3,4])\n",
    "    # set the font size of the ticks\n",
    "    ax.xaxis.set_tick_params(labelsize=7)\n",
    "    ax.yaxis.set_tick_params(labelsize=7)\n",
    "    \n",
    "    ax.set_title(r1_col.split('_')[1], fontsize=7)\n",
    "    ax.text(0.25, 0.8, r'$r^2$ =' + f'{r2:.3f}', transform=ax.transAxes, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save as one combined figure\n",
    "for form in ['png', 'svg']:\n",
    "    plt.savefig(f'{plot_folder}/1.1_interreplicate_correlation/1_combined_count_figure.{form}', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 - Start data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all columns that contain \"log2FoldChange\"\n",
    "log2fc_cols = [col for col in count_df.columns if 'log2FoldChange' in col]\n",
    "\n",
    "# get all columns that contain \"lfcSE\"\n",
    "lfcSE_cols = [col for col in count_df.columns if 'lfcSE' in col]\n",
    "\n",
    "log2_df = count_df[log2fc_cols]\n",
    "std_df = count_df[lfcSE_cols]\n",
    "\n",
    "# convert log2 to log10\n",
    "log10_df = log2_df.applymap(lambda x: np.log10(2**x))\n",
    "std_df = std_df.applymap(lambda x: np.log10(2**x))\n",
    "\n",
    "# for all columns names, split by \"_\", then remove the last element\n",
    "# and join them with a space\n",
    "log10_df.columns = ['_'.join(col.split('_')[:-1]) for col in log10_df.columns]\n",
    "std_df.columns = ['_'.join(col.split('_')[:-1]) for col in std_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide the data into dictionaries based on the design name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of all files in \"reference\" folder\n",
    "reference_files = os.listdir(\"../design_files/\")\n",
    "\n",
    "reference_dfs = {}\n",
    "for file in reference_files:\n",
    "    # if it's not a csv file, skip\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "    # get the name for the df from the file name\n",
    "    # it's the file name without the extension\n",
    "    name = file.split('.')[0]\n",
    "    reference_dfs[name] = pd.read_csv(\"../design_files/\" + file, index_col=0)\n",
    "\n",
    "# for each column, rename it according to label_rename if it's in the dictionary\n",
    "for key, df in reference_dfs.items():\n",
    "    df.rename(columns=label_rename, inplace=True)\n",
    "\n",
    "# for each dataframe, if there are columns that match cell lines, prepend predicted_ to the column name\n",
    "for key, df in reference_dfs.items():\n",
    "    for col in df.columns:\n",
    "        if col in cell_lines:\n",
    "            df.rename(columns={col: 'predicted_' + col}, inplace=True)\n",
    "\n",
    "reference_df_original = reference_dfs.copy()\n",
    "\n",
    "# for each dataframe, add the columns from results_df based on the index\n",
    "for key, df in reference_dfs.items():\n",
    "    reference_dfs[key] = reference_dfs[key].join(log10_df, how='left')\n",
    "    # drop rows with NaN values\n",
    "    reference_dfs[key].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in reference_dfs.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each reference dataframe, check which percentage of the indices are contained in the dataframe\n",
    "sum_original = 0\n",
    "sum_after = 0\n",
    "\n",
    "for key, df in reference_df_original.items():\n",
    "    print(f\"Percentage of {key} contained in results:\" + \\\n",
    "        str(100* df.index.isin(log10_df.index).sum() / len(df.index)))\n",
    "    sum_original += len(df)\n",
    "    sum_after += df.index.isin(log10_df.index).sum()\n",
    "    \n",
    "print(f\"Total number of rows in reference files: {sum_original}\")\n",
    "print(f\"Total number of rows in reference files after filtering: {sum_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show which entries are not contained in the results\n",
    "entries = []\n",
    "for key, df in reference_df_original.items():\n",
    "    entry = df.index[~df.index.isin(log10_df.index)]\n",
    "    if len(entry) > 0:\n",
    "        entries.append(entry.to_list())\n",
    "# flatten the list to get a valid index\n",
    "entries = [item for sublist in entries for item in sublist]\n",
    "\n",
    "# previously filtered designs:\n",
    "# contains AATAAA and was filtered during the design process\n",
    "# 1_mirna_full_single_high_conf_94\t\n",
    "# 1_mirna_full_single_high_conf_307\t\n",
    "# 1_mirna_full_single_high_conf_638\t\n",
    "# 1_mirna_full_single_high_conf_639\t\n",
    "# 2_mirna_full_single_low_conf_mirgenedb_165\t\n",
    "# 5.14_miRNA_let-7a-5p_smut_swob_omut11.15.19_owob\n",
    "\n",
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of polyA-containing designs\n",
    "polyA_designs = pd.read_excel('../design_info/polyA_signals.xlsx', index_col=0)\n",
    "polyA_designs\n",
    "\n",
    "# 94, 307, 638, and 639 were filtered beforehand (during library generation)\n",
    "# this means that only 3 designs are genuinely missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 - Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use microRNA data to normalize the expression for constructs that are not expected to be knocked down to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_expression_df = pd.read_csv(\"../microrna_data/3_output/Alles_Keller_completely_unfiltered_merge.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD EXPRESSION DATA TO THE DATAFRAMES\n",
    "# get all dfs that contain \"single\" in their key\n",
    "single_dfs = {key: reference_dfs[key].copy() for key in reference_dfs.keys() if \"single_high_conf\" in key or \"full_repeat\" in key}\n",
    "\n",
    "for key, df in single_dfs.items():\n",
    "    for cell_line in cell_lines:\n",
    "        # add miRNA expression data\n",
    "        # this is done by matching the column \"miRNA1\" in the df with the column \"miRNA\" in the mirna_expression_df\n",
    "        # not all values are present in the mirna_expression_df, so we have to match them\n",
    "        df[f\"{cell_line}_expression\"] = df[\"miRNA1\"].map(mirna_expression_df[cell_line])\n",
    "\n",
    "        # drop NaN values\n",
    "        df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "# create the plot folder if it doesn't exist\n",
    "if not os.path.exists(f'{plot_folder}/1.3_normalization'):\n",
    "    os.makedirs(f'{plot_folder}/1.3_normalization')\n",
    "\n",
    "# plot the data before normalization\n",
    "for key in single_dfs.keys():\n",
    "    for cell_line in cell_lines:\n",
    "        df = single_dfs[key]\n",
    "        knock_df = df[f\"{cell_line}_3UTR\"].sort_values(ascending=False)\n",
    "\n",
    "        plt.clf()\n",
    "        fig = plt.figure(figsize=(2.2, 1.7))\n",
    "\n",
    "        # create plots\n",
    "        plt.scatter(df.loc[knock_df.index, f\"{cell_line}_expression\"], 10**knock_df, s=5, color=\"dodgerblue\")\n",
    "        x_range = np.arange(0, 5.5, 0.01)\n",
    "        plt.plot(x_range, [1 for i in range(len(x_range))], color=\"black\", linestyle=\"dashed\", label=\"y=0\")\n",
    "\n",
    "        # calculate the correlation coefficient\n",
    "        r, p = stats.spearmanr(np.log10(df.loc[knock_df.index, f\"{cell_line}_expression\"]), knock_df)\n",
    "\n",
    "        plt.xlabel(r\"log$_{10}$\"+f\"({cell_line} expression)\")\n",
    "        plt.ylabel(r\"log$_{10}$(RNA/DNA)\")\n",
    "\n",
    "        plt.xlim(0, 5.5)\n",
    "        plt.legend(loc=\"lower left\", frameon=False)\n",
    "        plt.title(f\"{cell_line}_{key}, \" + r\"$\\rho^2$ = \" + str(round(r**2, 2)), fontsize=8)\n",
    "        plt.savefig(f\"{plot_folder}/1.3_normalization/{key}_{cell_line}_before.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in reference_dfs.items():\n",
    "    fig = plt.figure(figsize=(2.2, 1.7))\n",
    "    sns.violinplot(data=df[cell_line_labels], scale='width', inner='quartile', linewidth=0.5, palette='viridis')\n",
    "    plt.axhline(0, color='black', linestyle='dashed', linewidth=0.5)\n",
    "    # make cell_lines the xticklabels\n",
    "    plt.xticks(range(len(cell_lines)), cell_lines, rotation=90)\n",
    "    plt.ylabel(\"log10(RNA/DNA)\")\n",
    "    \n",
    "    plt.savefig(f\"{plot_folder}/1.3_normalization/violin_{key}_before.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we normalize to the expected value of single microRNA sites. We take the top 300 miRNAs as reference given that most miRNAs are not expressed in any given cell line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dfs for single microRNA target sites \n",
    "key = \"1_mirna_full_single_high_conf\"\n",
    "df_ctx1 = single_dfs[key]\n",
    "df_ctx1.dropna(inplace=True)\n",
    "\n",
    "df_results_norm = log10_df.copy()\n",
    "norm_factors = {}\n",
    "n_norm = 300\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "    knock_df_ctx1 = df_ctx1[f\"{cell_line}_3UTR\"].sort_values(ascending=False)\n",
    "    \n",
    "    # get the 200 highest values and calculate the median\n",
    "    norm_ctx1 = knock_df_ctx1.head(n_norm).median()\n",
    "    norm_factors[f\"{cell_line}\"] = norm_ctx1\n",
    "    \n",
    "    # subtract the median from all values to normalize them\n",
    "    # we subtract rather than divide because the values are log10 transformed\n",
    "    df_results_norm[f\"{cell_line}_3UTR\"] = df_results_norm.apply(lambda x: x[f\"{cell_line}_3UTR\"] - norm_ctx1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, we split by designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of all files in \"reference\" folder\n",
    "reference_files = os.listdir(\"../design_files/\")\n",
    "\n",
    "reference_dfs = {}\n",
    "for file in reference_files:\n",
    "    # check that the file ends in .csv\n",
    "    if file.endswith(\".csv\"):\n",
    "        # get the name for the df from the file name\n",
    "        # it's the file name without the extension\n",
    "        name = file.split('.')[0]\n",
    "        reference_dfs[name] = pd.read_csv(\"../design_files/\" + file, index_col=0)\n",
    "\n",
    "# for each column, rename it according to label_rename if it's in the dictionary\n",
    "for key, df in reference_dfs.items():\n",
    "    df.rename(columns=label_rename, inplace=True)\n",
    "\n",
    "# for each dataframe, if there are columns that match cell lines, prepend predicted_ to the column name\n",
    "for key, df in reference_dfs.items():\n",
    "    for col in df.columns:\n",
    "        if col in cell_lines:\n",
    "            df.rename(columns={col: 'predicted_' + col}, inplace=True)\n",
    "\n",
    "# for each dataframe, add the columns from results_df based on the index\n",
    "for key, df in reference_dfs.items():\n",
    "    reference_dfs[key] = reference_dfs[key].join(df_results_norm, how='left')\n",
    "    # drop rows with NaN values\n",
    "    reference_dfs[key].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the designs again after this initial normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD EXPRESSION DATA TO THE DATAFRAMES\n",
    "# get all dfs that contain \"single\" in their key\n",
    "single_dfs = {key: reference_dfs[key].copy() for key in reference_dfs.keys() if \"single_high_conf\" in key or \"full_repeat\" in key}\n",
    "\n",
    "for key, df in single_dfs.items():\n",
    "    for cell_line in cell_lines:\n",
    "        # add miRNA expression data\n",
    "        # this is done by matching the column \"miRNA1\" in the df with the column \"miRNA\" in the mirna_expression_df\n",
    "        # not all values are present in the mirna_expression_df, so we have to match them\n",
    "        df[f\"{cell_line}_expression\"] = df[\"miRNA1\"].map(mirna_expression_df[cell_line])\n",
    "\n",
    "        # drop NaN values\n",
    "        df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "# create the plot folder if it doesn't exist\n",
    "if not os.path.exists(f'{plot_folder}/1.3_normalization'):\n",
    "    os.makedirs(f'{plot_folder}/1.3_normalization')\n",
    "\n",
    "# plot the data after the first normalization step\n",
    "for key in single_dfs.keys():\n",
    "    for cell_line in cell_lines:\n",
    "        df = single_dfs[key]\n",
    "        knock_df = df[f\"{cell_line}_3UTR\"].sort_values(ascending=False)\n",
    "\n",
    "        plt.clf()\n",
    "        fig = plt.figure(figsize=(2.2, 1.7))\n",
    "\n",
    "        # create plots\n",
    "        plt.scatter(df.loc[knock_df.index, f\"{cell_line}_expression\"], 10**knock_df, s=5, color=\"dodgerblue\")\n",
    "        x_range = np.arange(0, 5.5, 0.01)\n",
    "        plt.plot(x_range, [1 for i in range(len(x_range))], color=\"black\", linestyle=\"dashed\", label=\"y=0\")\n",
    "\n",
    "        # calculate the correlation coefficient\n",
    "        r, p = stats.spearmanr(np.log10(df.loc[knock_df.index, f\"{cell_line}_expression\"]), knock_df)\n",
    "\n",
    "        plt.xlabel(r\"log$_{10}$\"+f\"({cell_line} expression)\")\n",
    "        plt.ylabel(r\"log$_{10}$(RNA/DNA)\")\n",
    "\n",
    "        plt.xlim(0, 5.5)\n",
    "        plt.legend(loc=\"lower left\", frameon=False)\n",
    "        plt.title(f\"{cell_line}_{key}, \" + r\"$\\rho^2$ = \" + str(round(r**2, 2)), fontsize=8)\n",
    "        plt.savefig(f\"{plot_folder}/1.3_normalization/{key}_{cell_line}_after1.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in reference_dfs.items():\n",
    "    fig = plt.figure(figsize=(2.2, 1.7))\n",
    "    sns.violinplot(data=df[cell_line_labels], scale='width', inner='quartile', linewidth=0.5, palette='viridis')\n",
    "    plt.axhline(0, color='black', linestyle='dashed', linewidth=0.5)\n",
    "    \n",
    "    # make cell_lines the xticklabels\n",
    "    plt.xticks(range(len(cell_lines)), cell_lines, rotation=90)\n",
    "    plt.ylabel(\"log10(RNA/DNA)\")\n",
    "    \n",
    "    plt.savefig(f\"{plot_folder}/1.3_normalization/violin_{key}_norm_step1.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second normalization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the distribution of values in each design type as a violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dfs_old = copy.deepcopy(reference_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_numbers = {i: [] for i in range(0, 7)}\n",
    "for key in reference_dfs.keys():\n",
    "    df = reference_dfs[key].copy()\n",
    "    mirna_number = sum([1 for column in df.columns if \"miRNA\" in column and column != \"miRNA\"])\n",
    "    mirna_numbers[mirna_number].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now denote the subsets that are well suited to normalization\n",
    "# these are the ones where we expect a significant number of sequences to have baseline stability\n",
    "normalization_by_number = {i: [] for i in range(0, 7)}\n",
    "for key in reference_dfs.keys():\n",
    "    df = reference_dfs[key].copy()\n",
    "    mirna_number = sum([1 for column in df.columns if \"miRNA\" in column and column != \"miRNA\"])\n",
    "    if mirna_number == 1:\n",
    "        if key.startswith(\"1_\") or key.startswith(\"2_\") or key.startswith(\"3_\"):\n",
    "            normalization_by_number[mirna_number].append(key)\n",
    "    else:\n",
    "        if \"full_repeat\" in key: # or \"mut_repeat\" in key:\n",
    "            normalization_by_number[mirna_number].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the microRNA expression data from notebook 3 - we use an unfiltered geometric mean of the two main datasets\n",
    "mirna_expression = pd.read_csv(\"../microrna_data/3_output/Alles_Keller_completely_unfiltered_merge.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dicts_before = {}\n",
    "median_dicts_after = {}\n",
    "\n",
    "for mirna_number in mirna_numbers.keys():\n",
    "    curr_median_dict_before = {}\n",
    "    curr_median_dict_after = {}\n",
    "    \n",
    "    # don't normalize the control sequences\n",
    "    if mirna_number == 0:\n",
    "        continue\n",
    "    \n",
    "    # get the the well-suited designs\n",
    "    curr_dfs = []\n",
    "    for key in normalization_by_number[mirna_number]:\n",
    "        df = reference_dfs[key].copy()\n",
    "\n",
    "        # skip files that have different context sequences [we later want to examine how they differ]\n",
    "        if \"4_miRNA_full_single_context\" in key:\n",
    "            continue\n",
    "        \n",
    "        # only use lowly expressed miRNAs\n",
    "        if \"orig_mi\" in df.columns:\n",
    "            df.set_index(\"orig_mi\", inplace=True)\n",
    "        else:\n",
    "            df.set_index(\"miRNA1\", inplace=True)\n",
    "            \n",
    "        common_index = df.index.intersection(mirna_expression.index)\n",
    "        df = df.loc[common_index]\n",
    "        curr_dfs.append(df)  \n",
    "    curr_dfs = pd.concat(curr_dfs).reset_index(drop=True)\n",
    "    \n",
    "    # get the normalization factor for each cell line\n",
    "    median_dict = {}\n",
    "    for cell_line in cell_line_labels:\n",
    "        df = curr_dfs[cell_line].copy().dropna()\n",
    "        # sort the values from highest to lowest\n",
    "        df = df.sort_values(ascending=False)\n",
    "        # get the top 15%\n",
    "        df = df.head(int(0.15*len(df)))\n",
    "        # drop values larger than 2\n",
    "        df = df[df < 2]\n",
    "        \n",
    "        median = df.median()\n",
    "        median_dict[cell_line] = median\n",
    "        curr_median_dict_before[cell_line] = median\n",
    "        \n",
    "    # get the median of medians\n",
    "    median_of_medians = np.median(list(median_dict.values()))\n",
    "    \n",
    "    # apply the normalization\n",
    "    for key in mirna_numbers[mirna_number]:\n",
    "        df = reference_dfs[key].copy()\n",
    "        \n",
    "        for cell_line in cell_line_labels:\n",
    "            df[cell_line] = df[cell_line] - median_dict[cell_line] + median_of_medians\n",
    "\n",
    "        reference_dfs[key] = df\n",
    "    \n",
    "    # ----------------- AFTER NORMALIZATION -----------------\n",
    "    # This is to check if the normalization worked as intended\n",
    "    # get the the well-suited designs\n",
    "    curr_dfs = []\n",
    "    for key in normalization_by_number[mirna_number]:\n",
    "        df = reference_dfs[key].copy()\n",
    "\n",
    "        # skip files that have different context sequences [we later want to examine how they differ]\n",
    "        if \"4_miRNA_full_single_context\" in key:\n",
    "            continue\n",
    "        \n",
    "        # only use lowly expressed miRNAs\n",
    "        if \"orig_mi\" in df.columns:\n",
    "            df.set_index(\"orig_mi\", inplace=True)\n",
    "        else:\n",
    "            df.set_index(\"miRNA1\", inplace=True)\n",
    "            \n",
    "        common_index = df.index.intersection(mirna_expression.index)\n",
    "        df = df.loc[common_index]\n",
    "        curr_dfs.append(df)  \n",
    "    curr_dfs = pd.concat(curr_dfs).reset_index(drop=True)\n",
    "    \n",
    "    # get the median after\n",
    "    for cell_line in cell_line_labels:\n",
    "        df = curr_dfs[cell_line].copy().dropna()\n",
    "        # sort the values from highest to lowest\n",
    "        df = df.sort_values(ascending=False)\n",
    "        \n",
    "        # get the top 15%\n",
    "        df = df.head(int(0.15*len(df)))\n",
    "        \n",
    "        median = df.median()\n",
    "        curr_median_dict_after[cell_line] = median\n",
    "    \n",
    "    median_dicts_before[mirna_number] = curr_median_dict_before\n",
    "    median_dicts_after[mirna_number] = curr_median_dict_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note designs with very high stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = 0\n",
    "high_stability_designs = {}\n",
    "for key in reference_dfs.keys():\n",
    "    if key.startswith(\"4_\") or key.startswith(\"0_\"):\n",
    "        continue\n",
    "    \n",
    "    df = reference_dfs[key].copy()\n",
    "    \n",
    "    # get the linear maximum value across cell lines\n",
    "    cell_lines_UTR = [col for col in df.columns if \"3UTR\" in col]\n",
    "    curr_max = (10**df[cell_lines_UTR]).max(axis=1)\n",
    "    \n",
    "    # find all those that have a max value of more than 1.5\n",
    "    mask = curr_max > 1.5\n",
    "    print(f\"{key}, total designs: {mask.sum()}, percentage: {100*mask.sum()/len(mask)}\")\n",
    "    total_sum += mask.sum()\n",
    "    \n",
    "    high_stability_designs[key] = df[mask]\n",
    "    \n",
    "high_stability_designs = pd.concat(high_stability_designs.values())\n",
    "\n",
    "high_stability_folder = os.path.join(data_output_folder, \"high_stability\")\n",
    "if not os.path.exists(high_stability_folder):\n",
    "    os.makedirs(high_stability_folder)\n",
    "    \n",
    "high_stability_designs.to_csv(os.path.join(high_stability_folder, \"high_stability.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the normalized data for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each reference df as a csv\n",
    "for key, df in reference_dfs.items():\n",
    "    df.to_csv(os.path.join(data_output_folder, f\"{key}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the normalized data as a single dataframe\n",
    "threeUTRcolumns = [col for col in reference_dfs[\"0_lib2_controls\"].columns if \"3UTR\" in col]\n",
    "all_dfs = [reference_dfs[key][threeUTRcolumns] for key in reference_dfs.keys()]\n",
    "all_dfs = pd.concat(all_dfs)\n",
    "all_dfs.columns = [col.replace(\"_3UTR\", \"\") for col in all_dfs.columns]\n",
    "all_dfs = 10**all_dfs\n",
    "all_dfs.to_csv(os.path.join(data_output_folder, \"library2_normalized_stability_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### individual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD EXPRESSION DATA TO THE DATAFRAMES\n",
    "# get all dfs that contain \"single\" in their key\n",
    "single_dfs = {key: reference_dfs[key].copy() for key in reference_dfs.keys() if \"single_high_conf\" in key or \"full_repeat\" in key}\n",
    "\n",
    "for key, df in single_dfs.items():\n",
    "    for cell_line in cell_lines:\n",
    "        # add miRNA expression data\n",
    "        # this is done by matching the column \"miRNA1\" in the df with the column \"miRNA\" in the mirna_expression_df\n",
    "        # not all values are present in the mirna_expression_df, so we have to match them\n",
    "        df[f\"{cell_line}_expression\"] = df[\"miRNA1\"].map(mirna_expression[cell_line])\n",
    "\n",
    "        # drop NaN values\n",
    "        df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "# create the plot folder if it doesn't exist\n",
    "if not os.path.exists(f'{plot_folder}/1.3_normalization'):\n",
    "    os.makedirs(f'{plot_folder}/1.3_normalization')\n",
    "\n",
    "# plot the data after the second normalization step\n",
    "for key in single_dfs.keys():\n",
    "    for cell_line in cell_lines:\n",
    "        df = single_dfs[key]\n",
    "        knock_df = df[f\"{cell_line}_3UTR\"].sort_values(ascending=False)\n",
    "\n",
    "        plt.clf()\n",
    "        fig = plt.figure(figsize=(2.2, 1.7))\n",
    "\n",
    "        # create plots\n",
    "        plt.scatter(df.loc[knock_df.index, f\"{cell_line}_expression\"], 10**knock_df, s=5, color=\"dodgerblue\")\n",
    "        x_range = np.arange(0, 5.5, 0.01)\n",
    "        plt.plot(x_range, [1 for i in range(len(x_range))], color=\"black\", linestyle=\"dashed\", label=\"y=0\")\n",
    "\n",
    "        # calculate the correlation coefficient\n",
    "        r, p = stats.spearmanr(np.log10(df.loc[knock_df.index, f\"{cell_line}_expression\"]), knock_df)\n",
    "\n",
    "        plt.xlabel(r\"log$_{10}$\"+f\"({cell_line} expression)\")\n",
    "        plt.ylabel(r\"log$_{10}$(RNA/DNA)\")\n",
    "\n",
    "        plt.xlim(0, 5.5)\n",
    "        plt.legend(loc=\"lower left\", frameon=False)\n",
    "        plt.title(f\"{cell_line}_{key}, \" + r\"$\\rho^2$ = \" + str(round(r**2, 2)), fontsize=8)\n",
    "        for format in ['svg', 'png']:\n",
    "            plt.savefig(f\"{plot_folder}/1.3_normalization/{key}_{cell_line}_after2.{format}\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in reference_dfs.items():\n",
    "    fig = plt.figure(figsize=(2.2, 1.7))\n",
    "    sns.violinplot(data=df[cell_line_labels], scale='width', inner='quartile', linewidth=0.5, palette='viridis')\n",
    "    # plot a line at 0\n",
    "    plt.axhline(0, color='black', linestyle='dashed', linewidth=0.5)\n",
    "    \n",
    "    # make cell_lines the xticklabels\n",
    "    plt.xticks(range(len(cell_lines)), cell_lines, rotation=90)\n",
    "    plt.ylabel(\"log10(RNA/DNA)\")\n",
    "    for format in ['svg', 'png']:\n",
    "        plt.savefig(f\"{plot_folder}/1.3_normalization/violin_{key}_norm_step2.{format}\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 - Analyze change after median normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot folder if it doesn't exist\n",
    "if not os.path.exists(f'{plot_folder}/1.4_median_analysis'):\n",
    "    os.makedirs(f'{plot_folder}/1.4_median_analysis')\n",
    "\n",
    "for key, val in median_dicts_after.items():\n",
    "    val_after = val[list(val.keys())[0]]\n",
    "    index = list(median_dicts_before[key].keys())\n",
    "    index = [entry.split('_')[0] for entry in index]\n",
    "    values = list(median_dicts_before[key].values())\n",
    "    vals_before = pd.Series(values, index=index)\n",
    "    vals = val_after - vals_before\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(2.2, 2))\n",
    "    plt.bar(vals.index, vals)\n",
    "    \n",
    "    # rotate xticks 90 degrees\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"median after - median before\")\n",
    "    plt.ylim(-0.4, 0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_folder}/1.4_median_analysis/{key}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low value implies that the median before was higher, which in turn implies that constructs were relatively more stable than the baseline stability for constructs with a single target site for a non-expressed microRNA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences are small for values below AND4. This implies that the reference stability for some cell lines changes when sites 4 to 6 are introduced.\n",
    "Because we are interested in the stability relative to a case without microRNA knockdown (as opposed to the stability relative to the original context sequence without microRNA sequences), we divide this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vals = {}\n",
    "index = list(median_dicts_before[key].keys())\n",
    "index = [entry.split('_')[0] for entry in index]\n",
    "\n",
    "change_vals = {}\n",
    "for key in median_dicts_after.keys():\n",
    "    vals_after = pd.Series(list(median_dicts_after[key].values()), index=index)\n",
    "    vals_before = pd.Series(list(median_dicts_before[key].values()), index=index)\n",
    "    change_vals[key] = val_after - vals_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vals[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the change values\n",
    "relevant_keys = [4]\n",
    "average_change_vals = {cell_line: 0 for cell_line in cell_lines}\n",
    "for key in relevant_keys:\n",
    "    for cell_line in cell_lines:\n",
    "        average_change_vals[cell_line] += change_vals[key][cell_line]\n",
    "average_change_vals = {key: val/len(relevant_keys) for key, val in average_change_vals.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
